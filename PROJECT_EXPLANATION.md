# Automated Insight Engine - Complete Project Explanation

## ğŸ“‹ Project Overview

The **Automated Insight Engine (AIE)** is a web-based machine learning application built with **Streamlit** that automatically analyzes datasets and builds baseline ML models without requiring any coding. It's designed for data scientists and analysts who want to quickly understand their data and test different ML models.

### Key Purpose
- Upload a CSV file
- Automatically understand the data
- Clean the data
- Test multiple ML models
- Get performance comparisons
- Make predictions

---

## ğŸ—ï¸ Project Architecture - Layered Design

The project is organized into **7 main layers**, each handling a specific phase of the ML workflow:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INTERFACE (Streamlit)             â”‚
â”‚                         (app.py)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Layer 1: UNDERSTANDING    â”‚  Layer 2: DIAGNOSIS  â”‚
    â”‚  (understanding.py)        â”‚  (diagnostics.py)    â”‚
    â”‚                            â”‚                      â”‚
    â”‚  - Detect data types      â”‚  - Missing values    â”‚
    â”‚  - Find target variable   â”‚  - Outliers          â”‚
    â”‚  - Identify ML task       â”‚  - Duplicates        â”‚
    â”‚  - Check imbalance        â”‚  - Correlations      â”‚
    â”‚  - Flag data leakage      â”‚                      â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚         Layer 3: CLEANING                  â”‚
    â”‚         (cleaning.py)                      â”‚
    â”‚                                            â”‚
    â”‚  - Remove duplicates                       â”‚
    â”‚  - Fill missing values                     â”‚
    â”‚  - Remove sparse columns                   â”‚
    â”‚  - Cap outliers                            â”‚
    â”‚  - Create missing indicators               â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Layer 4: FEATURE SELECTION         â”‚
    â”‚         (selection.py)                     â”‚
    â”‚                                            â”‚
    â”‚  - Filter methods (Variance, Mutual Info)  â”‚
    â”‚  - Embedded methods (Random Forest, L1)    â”‚
    â”‚  - Wrapper methods (RFE)                   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Layer 5: MODEL SELECTION           â”‚
    â”‚         (models.py)                        â”‚
    â”‚                                            â”‚
    â”‚  - Logistic Regression                     â”‚
    â”‚  - Decision Trees                          â”‚
    â”‚  - Random Forest                           â”‚
    â”‚  - XGBoost & LightGBM                      â”‚
    â”‚  - KNN, SVM, Neural Networks               â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Layer 6: ORCHESTRATION             â”‚
    â”‚         (runner.py)                        â”‚
    â”‚                                            â”‚
    â”‚  - Prepare features for training           â”‚
    â”‚  - Encode targets                          â”‚
    â”‚  - Coordinate pipeline                     â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Layer 7: EVALUATION                â”‚
    â”‚         (evaluation.py)                    â”‚
    â”‚                                            â”‚
    â”‚  - Cross-validation                        â”‚
    â”‚  - Performance metrics                     â”‚
    â”‚  - Leaderboard ranking                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ File-by-File Explanation

### 1ï¸âƒ£ **understanding.py** - Data Understanding Layer

**What it does:** Analyzes the structure of your data to understand what you're working with.

**Key Functions:**

#### `infer_schema(df)`
- Detects column data types: numeric, categorical, datetime, boolean
- Separates different types of data for appropriate handling
- Identifies text columns that are actually numbers

**Example:**
```python
# Input: DataFrame with mixed columns
# Output: 
# - numeric: ['age', 'salary', 'score']
# - categorical: ['color', 'department', 'city']
# - datetime: ['birth_date', 'join_date']
```

#### `infer_target(df)` & `infer_task(df, target)`
- Automatically finds which column is the target variable
- Determines if it's a **classification** task (predicting categories) or **regression** task (predicting numbers)
- Examples:
  - Classification: Predicting "Yes/No", "Cat/Dog", "A/B/C"
  - Regression: Predicting house price, temperature, salary

#### `detect_imbalance(y, task, threshold=0.8)`
- Checks if one class has way more samples than others
- Important because models struggle when classes are imbalanced
- Example: If 95% samples are "No" and 5% are "Yes", that's imbalanced

#### `detect_leakage(X, y, task)`
- Flags features that are too correlated with the target
- These can cause overfitting (model learns coincidences, not patterns)
- Example: If predicting house price and a feature is "appraised_value", it's data leakage

**Output Structure:** `UnderstandingSummary` - contains all findings about your data

---

### 2ï¸âƒ£ **diagnostics.py** - Data Diagnosis Layer

**What it does:** Creates a "health report" of your data before cleaning.

**Key Functions:**

#### `missing_values_report(df)`
- Shows which columns have missing values (NaN)
- Reports the percentage missing
- Creates a visual heatmap of missing patterns
- Helps identify if missingness is random or systematic

**Example Output:**
```
Column          | Missing Count | Missing %
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
age             | 5             | 2.5%
email           | 45            | 22.5%
phone           | 180           | 90%
```

#### `duplicate_report(df)`
- Counts how many exact duplicate rows exist
- Calculates duplicate percentage
- Important: Duplicates can skew model performance

#### `outlier_report(df, numeric_cols)`
- Detects unusual values in numeric columns using IQR (Interquartile Range) method
- IQR = distance between 25th and 75th percentile
- Values beyond 1.5 Ã— IQR are flagged as outliers
- Examples: A salary of $10M when average is $100K

#### `correlation_analysis(df)`
- Finds numeric columns that are highly correlated
- Pearson correlation: measures linear relationship (-1 to +1)
- High correlation means two features have redundant information
- Example: "Height in cm" and "Height in inches" are perfectly correlated

#### `categorical_correlation(df, cat_cols)`
- Finds categorical columns that are related using Cramer's V test
- Similar to correlation but for category data
- Example: "Country" and "Language" might be correlated

**Output Structure:** `DiagnosisSummary` - contains all data health findings

---

### 3ï¸âƒ£ **cleaning.py** - Data Cleaning Layer

**What it does:** Automatically fixes data quality issues. Works like sklearn transformers (can be chained in pipelines).

**Key Classes:**

#### `DuplicateRemover`
- Removes exact duplicate rows
- Keeps first occurrence, deletes rest

#### `NumericImputer`
- Fills missing values in numeric columns
- Strategies:
  - **Median**: Takes middle value (robust to outliers)
  - **Mean**: Takes average (affected by outliers)
  - **KNN**: Uses K nearest neighbors' values (sophisticated)

**Example:**
```
Original: [1, 2, NaN, 4, 5]
After imputation (median): [1, 2, 3, 4, 5]
```

#### `CategoricalImputer`
- Fills missing values in text/category columns
- Default: Replaces with "Unknown"
- Can also use mode (most common value)

#### `OutlierCapper`
- Reduces impact of outliers by capping them
- Example: Values > 99th percentile set to 99th percentile value
- Prevents extreme values from distorting models

#### `SparsityDropper`
- Removes columns with too many missing values
- Default: Drop if >50% missing
- These columns have little usable information

#### `MissingIndicator`
- Creates new binary columns marking where data was missing
- Useful because "missingness itself" can be predictive
- Example: If a customer didn't provide phone number, they might be less engaged

**Example Cleaning Pipeline:**
```
Raw Data
  â†“
Remove Duplicates
  â†“
Drop Sparse Columns (>50% missing)
  â†“
Fill Missing Numbers (median)
  â†“
Fill Missing Categories (Unknown)
  â†“
Cap Outliers
  â†“
Create Missing Indicators
  â†“
Clean Data (ready for models)
```

---

### 4ï¸âƒ£ **selection.py** - Feature Selection Layer

**What it does:** Identifies which features (columns) are actually useful for predictions.

**Why it matters:** Using unnecessary features:
- Slows down training
- Makes models overfit (memorize noise)
- Makes models harder to understand

**Three Main Approaches:**

#### A. **Filter Methods** - Fast statistical approach
- **Variance Threshold**: Removes features with very low variance
- **Mutual Information**: Measures how much a feature tells about the target
- **Chi-Square**: For categorical features, tests relationship with target

**Simple analogy:** If a feature is nearly constant (always same value), it doesn't help prediction.

#### B. **Embedded Methods** - Built into the model
- **L1 Regularization (Lasso)**: Linear model that naturally removes weak features
- **Random Forest Importance**: Ranks features by how often they split the data

**How it works:** Random Forest trains and measures how much each feature improves decisions

#### C. **Wrapper Methods** - Test subsets
- **RFE (Recursive Feature Elimination)**: 
  1. Train model with all features
  2. Remove weakest feature
  3. Repeat until desired number remains

**Trade-off:** Wrapper is slowest but most accurate

**Output:** List of selected features + importance scores

---

### 5ï¸âƒ£ **models.py** - Model Zoo Layer

**What it does:** Provides a library of 9 classification and 8 regression models with tuned parameters.

**Classification Models** (predict categories):

| Model | When to Use | Pros | Cons |
|-------|-------------|------|------|
| **Logistic Regression** | Baseline, interpretable | Fast, simple | Limited for complex patterns |
| **Decision Tree** | Interpretable | Handles non-linearity, no scaling needed | Overfits easily |
| **Random Forest** | Most robust | Powerful, handles nonlinearity | Less interpretable |
| **KNN** | Small datasets | Simple, adapts well | Slow on large data |
| **SVM** | High-dimensional data | Powerful, memory efficient | Slow to train |
| **XGBoost** | Best performance | Extremely powerful, fast | Complex, needs tuning |
| **LightGBM** | Huge datasets | Very fast | Less stable than XGBoost |
| **Neural Network (MLP)** | Complex patterns | Universal approximator | Black box, needs lots of data |
| **Naive Bayes** | Text/spam | Simple, fast | Assumes independence |

**Regression Models** (predict numbers):

| Model | Purpose |
|-------|---------|
| Linear Regression | Baseline |
| Decision Tree Regressor | Non-linear baseline |
| Random Forest Regressor | Robust predictions |
| KNN Regressor | Local average |
| SVM Regressor | Non-linear relationships |
| XGBoost Regressor | High-precision predictions |
| LightGBM Regressor | Fast large-scale |
| Neural Network Regressor | Complex patterns |

**Model Selection Logic:**
```
if dataset is small (< 1000 rows):
    use simpler models (Logistic Regression, Decision Tree)
else if dataset is medium (1000-100K rows):
    use balanced models (Random Forest, XGBoost)
else if dataset is huge (>100K rows):
    use fast models (LightGBM, Linear Regression)
    
if many categorical features:
    prefer tree-based models (Random Forest, XGBoost)
else:
    can use any model
```

---

### 6ï¸âƒ£ **runner.py** - Orchestration Layer

**What it does:** Coordinates the entire ML pipeline. Prepares data for training.

**Key Functions:**

#### `encode_classification_target(y)`
- Converts text labels to numbers (required by models)
- Example: "Yes" â†’ 1, "No" â†’ 0, "Maybe" â†’ 2
- Stores the mapping for later decoding

#### `prepare_features(df, target)`
- Separates features (X) from target (y)
- Removes ID columns (id, identifier, employee_id) - they don't help
- Removes near-unique columns (99% unique values = essentially IDs)
- Converts text to numbers

**Example:**
```
Input DataFrame:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ID â”‚ Age      â”‚ City â”‚ Purchased â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚ 25       â”‚ NYC  â”‚ Yes       â”‚
â”‚ 2  â”‚ 30       â”‚ LA   â”‚ No        â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output:
X (Features):           y (Target):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age      â”‚ City â”‚    â”‚ Purchased â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 25       â”‚ 0    â”‚    â”‚ 1         â”‚
â”‚ 30       â”‚ 1    â”‚    â”‚ 0         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

(ID dropped, City converted to numbers)
```

---

### 7ï¸âƒ£ **evaluation.py** - Evaluation & Leaderboard Layer

**What it does:** Tests models and creates a ranking (leaderboard) of their performance.

**Key Metrics:**

#### Classification Metrics (for Yes/No type predictions):

- **Accuracy**: % of correct predictions
  - Formula: Correct Predictions / Total Predictions
  - Example: 95% accuracy = 95 out of 100 correct
  
- **Precision**: Of positive predictions, how many are actually positive
  - Formula: True Positives / (True Positives + False Positives)
  - Use when: False positives are expensive
  - Example: Email spam filter - precision = "of emails marked spam, how many really are spam"

- **Recall**: Of actual positives, how many did we find
  - Formula: True Positives / (True Positives + False Negatives)
  - Use when: False negatives are expensive
  - Example: Cancer detection - recall = "of actual cancer cases, how many did we catch"

- **F1-Score**: Harmonic mean of precision & recall
  - Good for imbalanced datasets
  - Balances both concerns

- **ROC-AUC**: Area under ROC curve
  - Measures discrimination ability across all thresholds
  - Range: 0.5 (random) to 1.0 (perfect)

#### Regression Metrics (for number predictions):

- **RMSE** (Root Mean Square Error): Average prediction error
  - Example: RMSE $5000 means predictions off by ~$5000 on average
  
- **MAE** (Mean Absolute Error): Average absolute difference
  - More interpretable than RMSE
  
- **RÂ²**: How much variance is explained (0 to 1)
  - RÂ² = 0.8 means model explains 80% of variation
  
- **MAPE**: Mean Absolute Percentage Error
  - Good for comparing datasets with different scales
  - Example: 15% MAPE means 15% off on average

#### `cross_validate_model(model, X, y, cv_folds=5)`
- Tests model 5 times on different data subsets
- **K-Fold Cross-Validation:**
  1. Split data into 5 equal parts
  2. Use 4 parts to train, 1 to test
  3. Repeat 5 times, each part as test set once
  4. Average the scores
  
- **Why?** Gives realistic estimate of performance on new data

```
Fold 1: Train on [2,3,4,5]  Test on [1]
Fold 2: Train on [1,3,4,5]  Test on [2]
Fold 3: Train on [1,2,4,5]  Test on [3]
Fold 4: Train on [1,2,3,5]  Test on [4]
Fold 5: Train on [1,2,3,4]  Test on [5]

Final Score = Average of all 5 test scores
```

#### `create_leaderboard(models, X, y, task)`
- Trains all models and ranks them
- Shows which model performed best
- Creates bar chart visualization

---

### 8ï¸âƒ£ **app.py** - User Interface Layer (Streamlit)

**What it does:** Web interface where users interact with everything above.

**Architecture:**
```
User Uploads CSV
    â†“
[Tab 1: Data Understanding]
    â†“ Displays schema, target, task type
    â†“ Shows imbalance, skewness, leakage
    â†“
[Tab 2: Diagnosis]
    â†“ Missing values heatmap
    â†“ Outliers, duplicates
    â†“ Correlation analysis
    â†“
[Tab 3: Cleaning]
    â†“ Apply cleaning transformations
    â†“ Choose strategies
    â†“ Preview cleaned data
    â†“
[Tab 4: Features]
    â†“ Feature importance
    â†“ Select important features
    â†“
[Tab 5: Models]
    â†“ Available models for this task
    â†“
[Tab 6: Leaderboard]
    â†“ Cross-validate all models
    â†“ Show rankings
    â†“
[Tab 7: Train & Test]
    â†“ Train selected model
    â†“ Show detailed metrics
    â†“ Performance summary
```

**Key Functions in app.py:**

#### `prepare_features(df, target_col)`
- Converts categorical to numbers using one-hot encoding
- Sanitizes column names for LightGBM compatibility
- Returns X (features) and y (target)

#### `get_available_models(task, n_rows, n_features, n_categorical)`
- Selects models based on:
  - Dataset size
  - Number of features
  - Number of categorical columns
- Returns appropriate model candidates

#### `generate_prediction_summary(model_name, task, metrics)`
- Creates human-readable interpretation of model results
- Example: "Model achieved 95% accuracy meaning..."

---

## ğŸ”„ Complete Data Pipeline Flow

### Step 1: User Uploads Data
```
CSV File â†’ Streamlit App â†’ Loaded as DataFrame
```

### Step 2: Data Understanding
```
DataFrame
  â†“
infer_schema() â†’ Detect types
  â†“
infer_target() â†’ Find target column
  â†“
infer_task() â†’ Classification or Regression?
  â†“
detect_imbalance() â†’ Check class balance
  â†“
detect_skewness() â†’ Check numeric distribution
  â†“
detect_leakage() â†’ Flag suspicious correlations
  â†“
Display: Schema, Task, Issues
```

### Step 3: Data Diagnosis
```
DataFrame
  â†“
missing_values_report() â†’ Show NaN patterns
  â†“
duplicate_report() â†’ Count duplicates
  â†“
outlier_report() â†’ Find unusual values
  â†“
correlation_analysis() â†’ Numeric relationships
  â†“
categorical_correlation() â†’ Categorical relationships
  â†“
Display: Heatmaps, Statistics, Visualizations
```

### Step 4: Data Cleaning
```
DataFrame
  â†“
Remove Duplicates
  â†“
Drop Sparse Columns (>50% missing)
  â†“
Impute Missing Values
  â”‚  â”œâ”€ Numeric: Median strategy
  â”‚  â””â”€ Categorical: "Unknown" strategy
  â†“
Cap Outliers (limit extreme values)
  â†“
Create Missing Indicators (track what was missing)
  â†“
Clean DataFrame
```

### Step 5: Feature Engineering
```
Clean DataFrame
  â†“
Create Interaction Features (optional)
  â†“
Feature Selection:
  â”œâ”€ Variance Threshold
  â”œâ”€ Mutual Information
  â”œâ”€ Chi-Square Test
  â”œâ”€ Random Forest Importance
  â””â”€ L1 Regularization
  â†“
Selected Features List
```

### Step 6: Data Preparation for Models
```
Clean DataFrame + Selected Features
  â†“
Separate X (features) and y (target)
  â†“
Remove ID columns
  â†“
Encode Categorical Variables
  â”‚  â”œâ”€ One-hot encoding (in app)
  â”‚  â””â”€ Factorization (in runner)
  â†“
Encode Target (for classification)
  â†“
Ready for Model Training
```

### Step 7: Model Training & Cross-Validation
```
X, y (prepared data)
  â†“
For Each Model in Registry:
  â”œâ”€ Split into K-Folds (5 or 10)
  â”œâ”€ Train on fold 1,2,3,4
  â”œâ”€ Test on fold 5
  â”œâ”€ Calculate metrics
  â”œâ”€ Repeat for all folds
  â””â”€ Average metrics
  â†“
Model Performance Scores
```

### Step 8: Leaderboard Generation
```
All Model Scores
  â†“
Sort by Primary Metric
  â”œâ”€ Classification: F1-Score
  â””â”€ Regression: RÂ²
  â†“
Create Ranking
  â†“
Visualize Bar Charts
  â†“
Display Leaderboard
```

### Step 9: Final Training & Testing
```
Selected Model + Prepared Data
  â†“
Split: 80% train, 20% test
  â†“
Train on training set
  â†“
Predict on test set
  â†“
Calculate detailed metrics
  â†“
Generate performance summary
  â†“
Display:
  â”œâ”€ Metrics table
  â”œâ”€ Confusion matrix (classification)
  â”œâ”€ Feature importance
  â””â”€ Human-readable summary
```

---

## ğŸ¯ Example Workflow: Predicting Customer Churn

### User's Goal
Build a model to predict which customers will leave (churn).

### Step-by-Step Execution

**1. Upload Data**
```
Dataset: 1000 customers, 15 columns
Columns: age, tenure, monthly_bill, customer_service_calls, churn (Yes/No)
```

**2. Understanding**
```
Schema:
  - Numeric: age, tenure, monthly_bill, customer_service_calls
  - Categorical: gender, contract_type, internet_service
  - Target: churn (Classification task)
  
Findings:
  âœ“ 70% No, 30% Yes (imbalanced but manageable)
  âœ“ Some skewness in tenure (many new customers)
  âœ— monthly_bill highly correlated with tenure (expected)
```

**3. Diagnosis**
```
Issues Found:
  - 2% missing values in customer_service_calls
  - 15 duplicate customer records
  - 3 outliers in monthly_bill (customers paying $5000+)
  - High correlation between contract_type and monthly_bill
```

**4. Cleaning**
```
Actions Taken:
  âœ“ Removed 15 duplicate records
  âœ“ Filled 2% missing values with median
  âœ“ Capped extreme bills at 95th percentile
  âœ“ Created "has_missing_calls" indicator
  
Result: 985 clean records
```

**5. Feature Selection**
```
All Features Score:
  - tenure: 0.95 â˜…â˜…â˜…â˜…â˜… (most important)
  - monthly_bill: 0.87 â˜…â˜…â˜…â˜…
  - contract_type: 0.82 â˜…â˜…â˜…â˜…
  - customer_service_calls: 0.78 â˜…â˜…â˜…
  - internet_service: 0.65 â˜…â˜…â˜…
  - age: 0.45 â˜…â˜… (less important)
  
Selected: Top 5 features
```

**6. Model Leaderboard**
```
Rank | Model           | F1-Score | Accuracy
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 1   | XGBoost         | 0.85     | 0.89 âœ“
 2   | Random Forest   | 0.82     | 0.87
 3   | LightGBM        | 0.81     | 0.86
 4   | Logistic Reg    | 0.76     | 0.82
 5   | Decision Tree   | 0.72     | 0.79
```

**7. Final Training**
```
Selected: XGBoost (best F1-score)

Train/Test Split: 80% train (788), 20% test (197)

Test Results:
  - Accuracy: 89% (correctly identified 175/197 cases)
  - Precision: 87% (when we predict churn, 87% actually churn)
  - Recall: 83% (we catch 83% of churners)
  - F1-Score: 0.85 (excellent balance)
  
Top Predictors:
  1. tenure (most important)
  2. monthly_bill
  3. contract_type
```

**8. Insights Generated**
```
ğŸ“Š Business Insights:

âœ“ Tenure is strongest churn indicator
  â†’ Focus retention on first 6 months

âœ“ High bills predict churn
  â†’ Consider price-lock programs

âœ“ Contract type matters
  â†’ Month-to-month customers churn 5x more

âœ“ Service calls correlate with churn
  â†’ Better first-contact resolution needed

Model ready for:
  - Scoring new customers
  - Identifying at-risk segments
  - Measuring retention program impact
```

---

## ğŸš€ How to Run the Application

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the App
```bash
# Windows
.\run.bat

# macOS/Linux
./run.sh

# Or directly
streamlit run ui/app.py
```

### 3. In Browser
```
Open: http://localhost:8501
```

### 4. Upload CSV
- Click "Browse files"
- Select your CSV
- Wait for processing
- Explore tabs

---

## ğŸ”‘ Key Technologies

| Technology | Purpose |
|-----------|---------|
| **Streamlit** | Web interface (no HTML/CSS needed) |
| **Pandas** | Data manipulation & analysis |
| **Scikit-learn** | ML algorithms & metrics |
| **XGBoost** | Powerful gradient boosting |
| **LightGBM** | Fast gradient boosting |
| **Plotly** | Interactive visualizations |
| **Matplotlib** | Static visualizations |

---

## ğŸ’¡ Summary Table of Each Module

| Module | Layer | Input | Output | Key Algorithm |
|--------|-------|-------|--------|----------------|
| understanding.py | 1 | Raw DataFrame | Schema, Target, Task | Type inference |
| diagnostics.py | 2 | DataFrame | Missing, Outliers, Corr | Statistical analysis |
| cleaning.py | 3 | DataFrame | Clean DataFrame | Imputation, capping |
| selection.py | 4 | X, y | Feature list | MI, Random Forest |
| models.py | 5 | - | Model registry | Algorithm selection |
| runner.py | 6 | DataFrame | X, y prepared | Encoding, preprocessing |
| evaluation.py | 7 | Model, X, y | Scores, Leaderboard | K-Fold CV |
| app.py | 8 | User input | Web interface | Streamlit framework |

---

## ğŸ“ Learning Path

To understand the codebase:
1. **Start with app.py** - See what users do
2. **Read understanding.py** - How data is analyzed
3. **Read diagnostics.py** - What problems are found
4. **Read cleaning.py** - How problems are fixed
5. **Read selection.py** - How features are selected
6. **Read models.py** - What models are available
7. **Read evaluation.py** - How performance is measured
8. **Read runner.py** - How it all connects

---

## ğŸ† Why This Architecture?

âœ… **Modular**: Each layer is independent, easy to modify
âœ… **Reusable**: Layers can be used in other projects
âœ… **Testable**: Each module has unit tests
âœ… **Transparent**: Easy to understand what's happening
âœ… **Extensible**: Easy to add new models, metrics, cleaning strategies
âœ… **Production-Ready**: Can be deployed as API

---

This comprehensive explanation should help you understand every aspect of the Automated Insight Engine!
